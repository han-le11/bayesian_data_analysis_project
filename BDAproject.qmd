---
title: "Bayesian Data Analysis project"
subtitle: "Espoo housing price prediction"
bibliography: ref.bib
csl: ieee.csl
author: anonymous # <-- hand in anonymously
format:
  html:
    toc: true
    code-tools: true
    code-line-numbers: true
    number-sections: true
    mainfont: Georgia, serif
    page-layout: article
  pdf:  
    geometry:
    - left=3cm,top=1cm,bottom=1cm,right=3cm
    number-sections: true
    code-annotations: none
editor: source
link-citations: true
output:
  bookdown::html_document2: default
  bookdown::pdf_document2: default
---

:::: {.content-hidden when-format="pdf"}
::: {.callout-warning collapse=true}

## Setup
```{r}
#| label: imports
library(aaltobda)
library(bayesplot)
library(dplyr)
library(ggplot2)
library(ggdist) # for stat_dotsinterval
library(posterior)
library(brms)
library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)  
library(cmdstanr)
library(bookdown)
# Globally specfiy cmdstan backend for brms
options(brms.backend="cmdstanr")
# Tell brms to cache results if possible
options(brms.file_refit="on_change")

# Set more readable themes with bigger font for plotting packages.
ggplot2::theme_set(theme_minimal(base_size = 14))
bayesplot::bayesplot_theme_set(theme_minimal(base_size = 14))

# This registers CmdStan as the backend for compiling cmdstan-chunks.
# check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)
# register_knitr_engine(override = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo=FALSE) 
```
:::
::::

# Introduction
In February 2024, the Pellervo Economic Research (PTT) of Finland forecasts that the housing prices in Espoo will increase by 1.7\% because of the influx of people moving to Espoo @ptt2024. Prediction of housing prices helps individuals and businesses make informed decisions about buying, selling, or investing in housing properties. For people planning to buy a house in Espoo, housing price prediction helps with financial planning and estimating the mortgage. For real estate professionals, economists, and policymakers, housing price prediction provides insights into factors that influence housing supply and demand, as well as urban development patterns. For example, housing price predictive models can help identify areas with affordable housing options, address housing inequality, and promote inclusive urban development. In addition, for banks, mortgage lenders, and other financial organizations, predicting housing prices is essential for assessing the risk associated with lending and investment activities.

However, housing price prediction can be challenging. The relationships between housing attributes and prices may not be linear. Many factors such as housing age, size, and average income of housing area can affect house prices. In this project, our goal is to model the effects of Espoo housing size and age on their prices, using non-hierarchical linear models and hierarchical  models. Regarding the linear model, we investigate with two variables—the age and the size of the house. For the hierarchical model, we also add hierarchy by using average income of the postal area as a grouping variable.

The structure of the report is as follows. Section 2 describes the data and the analysis problem. Section 3 describes the models used for analysis and prior choices. Section 4 presents our analysis with the non-hierarchical linear model and Section 5 for the hierarchical model. Section 6 shows the results of comparison between our two models. Section 7 discusses issues and potential improvements. Section 8 concludes what was learned from the data analysis. Finally, Section 9 is our self-reflection of what we learned while making the project.


# Description of the data and the analysis

## General description
The housing price dataset is obtained from Asuntojen Hintatiedot @asunto, which can be translated into Price Information of Housing. This dataset can be viewed and downloaded from [here](https://drive.google.com/file/d/1VMD4fkEzAcP71FXoBq6P5i0Rme04AugG/view?usp=sharing). At the time of conducting the analysis and making this report, to our knowledge, there are no other existing analyses with this housing dataset.

In the original dataset, there are 901 observations and 10 variables. We filter out one house that ages over 100 years from the dataset; therefore, we do the analysis with 900 observations. Each variable contains certain information about a house. We also added two variables to use in our analysis. The first added variable is $Age = 2024 - ProductionYear$, which computes the age of a house based on its production year and is used to investigate the effect of house's age on its price. The second added variable is $IncomeClass$, which rounds the variable $Income$. Below is the first 5 rows of the dataset `HouseData` after adding two variables.


:::: {.content-hidden when-format="pdf"}
::: {.callout-warning collapse=true}

Hiding in the pdf report because not all are needed to show. Feel free to adjust!
```{r}
HouseData = read.csv('data/housing-data.csv', header=TRUE)

HouseData = HouseData %>% 
  mutate(Age = 2024 - ProductionYear, 
         IncomeClass = round(Income, digits = -2)
         ) %>% 
  filter(Age <= 80)
```

```{r}
# some plots of the raw data
hist(HouseData$IncomeClass)
hist(HouseData$Income)
hist(HouseData$Rooms)
hist(HouseData$Price)
```

```{r}
# ggplot(data = HouseData %>% filter(
#   Rooms <= 2, 
#   Age <= 80)) + 
#   geom_point(aes(Age,Price, color = BuildingType))
# 
# ggplot(data = HouseData %>% filter(
#   Rooms > 2, 
#   Age <= 80)) + 
#   geom_point(aes(Age,Price, color = BuildingType))
# 
# ggplot(data = HouseData %>% filter(
#   Age <= 80)) + 
#   geom_point(aes(Size,Rooms, color = BuildingType), alpha = 0.5)
```
:::
::::

```{r echo=FALSE}
head(HouseData, 5)
```

There are 8 empty cells in column \textit{Condition} and 30 in column \textit{LandOwnership}. Therefore, these two parameters are not used for the Bayesian models and analyses in this report. 


## Exploratory data analysis

In this part, we present how we use visualisation to learn more about the dataset `HouseData`. We plot some histograms to explore the range of Espoo housing price, size, and age. As observed in the histograms below, the housing price and size can be assumed to follow normal distribution. Most houses fall in the range of 50 to 100 square meter, with a few outliers of houses over 200 $m^2$. Most houses age from 0 to 10 years. Besides, all house sizes and ages in the dataset are positive values, as they should.

```{r hist_price, fig.cap="Price of houses in the Espoo housing dataset.", echo=FALSE}
hist_size = hist(HouseData$Price, 
                 main="Price of houses in Espoo (in EUR)",
                 xlab = "Price (in EUR)")
```

```{r hist_size, fig.cap="Size range of all houses in the Espoo housing dataset.", echo=FALSE}
hist_size = hist(HouseData$Size, 
                 main="Size of houses in Espoo",
                 xlab = "Square metre")
```
```{r hist_age, fig.cap = "Age range of houses in Espoo as of 2024", echo=FALSE}
hist_age = hist(HouseData$Age, 
                main="Age of houses in Espoo as of 2024",
                xlab = "Age (years)")
```

There are three building types in the dataset: apartment, detached, and row house. Since the building type can also affect the price, we group the houses by their building type and explore the overall relationship between housing price and age. The scatter plot in Figure suggests that overall, across the ages, apartment is the cheapest type, followed by row house. Detached house is the most expensive type. 

```{r scatter1, fig.width=6, fig.cap = "Housing price & age by building type.", echo=FALSE}
ggplot(data = HouseData %>% filter(
  Age <= 80)) + 
  geom_point(aes(Age, Price, color = BuildingType)) +
  scale_color_manual(labels=c("Apartment house", "Detached house", "Row house"), values = c("indianred", "darkseagreen", "steelblue")) +
  labs(
    x = "Age (years)"
  )
```


Figure below shows the housing size and price, grouped by three building types. We can observe that generally, apartments are in the cheapest and smallest size range, while detached houses are in the most expensive and largest size range. 

```{r scatter2, fig.width=6, fig.cap = "Housing price & size by building type"}
ggplot(data = HouseData %>% filter(
  Age <= 80)) + 
  geom_point(aes(Size, Price, color = BuildingType)) +
  scale_color_manual(labels=c("Apartment house", "Detached house", "Row house"), values = c("indianred", "darkseagreen", "steelblue")) + 
  labs(
    x = "Size (square meter)"
  )
```

:::: {.content-hidden when-format="pdf"}
::: {.callout-warning collapse=true}
Comment: Hiding because This plot for each postal code is a bit confusing and hard to read. Maybe you can take a look?
```{r}
# plots for each postal code
ggplot(data = HouseData %>% filter(
  Age <= 80)) + 
  geom_point(aes(Age, PricePerSquare, color = BuildingType)) + facet_wrap(~IncomeClass)
```
:::
:::: 


# Models and prior choices

From the exploratory data analysis and by intuition, we can see that there can be meaningful effect of housing size and age on its price. Therefore, we will model the effects of housing size and age in the dataset `HouseData` on their prices. As a common practice, we start our analysis with a simple, vanilla linear non-hierarchical model with Gaussian noise.

Next, to take into account the variability of house prices by area, we use the average incomes of the postal areas as the grouping variable for the hierarchical model. We chose to use the income data (instead of the postal codes themselves), because we wanted to aggregate some of the smaller postal areas with other areas and we assume that average income gives a good enough similarity measure.

In both models, we choose normal priors for the parameters with $(\mu, \sigma)$ equal to $(5000, 1500)$ for the price per square meter, $(-1000, 5000)$ for the yearly depreciation of house price and $(1e5, 3e4)$ for the intercept. A reasonable guess for the price per square meter is 5000 euros and we expect that it's not too far off so we set the standard deviation at 1500, meaning values under 0 and over 10000 are very unlikely by our prior knowledge. We estimate the yearly depreciation to be some negative number on the order of thousands of euros per year, so we set its prior mean at -1000 with a fairly large variance by setting the standard deviation at 5000 (because we expect that 50 year old houses, for example, aren't free). We set the intercept's prior mean at 100,000 euros & the standard deviation at 30,000 euros, because we expect that the cheapest new apartments cost about 100,000 euros, but aren't free no matter how small. 


# Analysis with the linear model

Overall, this section shows the code of our linear model and how the Markov chain Monte Carlo (MCMC) inference was run. We also shows the convergence diagnostic values for the linear model and their interpretation. In addition, we report posterior predictive checks and sensitivity analysis.

## MCMC inference

To fit the model and run MCMC inference, we use `brms`—a high-level interface for Stan providing tools to create a wide range of Bayesian models. By default, 4 chains were drawn with 2000 iterations for each chain. The warm-up length for each chain is 1000. 

```{r}
fit1 = brms::brm(Price ~ Size + Age,
           data = HouseData,
           family = gaussian(),
           prior = c(
             prior(normal(5000, 1500), class = b, coef = Size),
             prior(normal(-1000, 5000), class = b, coef = Age),
             prior(normal(100000, 30000), class = Intercept)
             ),
           refresh=0, 
           show_exceptions = FALSE,
           # This causes brms to cache the results
           file="fit1.rds" 
           )
```


:::: {.content-hidden when-format="pdf"}
::: {.callout-warning collapse=true}

```{r}
fit1 <- readRDS("fit1.rds")
saveRDS(fit1, file="fit1.rds")
```

:::: 
::: 

## Convergence diagnostic 

Below is the summary and convergence diagnostic report for our fitted linear model. $\hat{R}$ is computed to monitor the convergence of iterative simulation. For all variables our $\hat{R}$ are under 1.01, which indicates possible convergence and means that we can stop the sampling process. In case $\hat{R} > 1.01$, we need to keep sampling to reach convergence. All ESS ratios are over 50 percents, which means that the effective sample sizes are sufficient.

By using function `check_hmc_diagnostics()`, we can verify that none of 4000 iterations saturated the maximum tree depth of 10.

:::: {.content-hidden when-format="pdf"}
::: {.callout-warning collapse=true}
```{r echo=FALSE}
rstan::check_hmc_diagnostics(fit1$fit)
```
:::: 
:::


```{r}
rhat(fit1)
neff_ratio(fit1)
```

:::: {.content-hidden when-format="pdf"}
::: {.callout-warning collapse=true}
too hard to read so hide it in pdf
```{r echo=FALSE}
summary(fit1)
```
:::: 
:::

By using function `plot()`, we can plot the MCMC chains and the posterior distributions for each parameter. From the figure, we observe that our MCMC chains have converged and mixed well and to the same posterior.  

```{r, fig.height=6, fig.width=8, fig.cap="Four MCMC chains and posterior distributions for each parameter."}
plot(fit1)
```


## Posterior predictive check

To investigate and compare model fit, we can apply graphical posterior predictive checks (Figure 7). Let's check the posterior predictions compared to the observed data using the `pp_check` function. In the plot below, the dark blue curve represents the $y$ values, which are the observed data, and the light blue curves represent $y_{rep}$ values, which are replicated data sets from the posterior predictive distribution. Based on the plot, the posterior prediction roughly encapsulates the main features of the observed data. However, there are negative values $y_{rep}$ from the posterior predictive distribution, which means the positivity of house price is not captured.

```{r, check1, fig.cap = "Posterior predictive check for the non-hierarchical model", echo=FALSE}
brms::pp_check(fit1)
```

Next, in Figure 8, we use the `conditional_effects` method to visualize the model-implied linear relationship between housing size and price as well as housing age and price.

```{r, fig.height=3, fig.width=5, fig.cap="Conditional effects of housing attributes on housing price.", echo=FALSE}
plot(conditional_effects(fit1), points = TRUE, ask=FALSE)
```

## Sensitivity analysis


Sensitivity analysis is conducted with respect to prior choices. To keep the sensitivity analysis of the linear model simple and not too time-consuming, we slightly change the priors for all three parameters to see whether the results change a lot. 

```{r}
fit1a = brms::brm(Price ~ Size + Age,
           data = HouseData,
           family = gaussian(),
           prior = c(
             prior(normal(8000, 500), class = b, coef = Size),
             prior(normal(0, 1000), class = b, coef = Age),
             prior(normal(100000, 10000), class = Intercept)
             ),
           refresh=0, 
           show_exceptions = FALSE,
           # This causes brms to cache the results
           file="fit1a.rds" 
           )

fit1b = brms::brm(Price ~ Size + Age,
           data = HouseData,
           family = gaussian(),
           prior = c(
             prior(normal(1000, 500), class = b, coef = Size),
             prior(normal(-3000, 1000), class = b, coef = Age),
             prior(normal(50000, 10000), class = Intercept)
             ),
           refresh=0, 
           show_exceptions = FALSE,
           # This causes brms to cache the results
           file="fit1b.rds" 
           )

cf = data.frame(O = fixef(fit1)[,1])
cf$A = fixef(fit1a)[,1]
cf$B = fixef(fit1b)[,1]

print(cf)
```
The first column is parameter of our original linear model. The second and third column are for the linear models with modified priors. We see that changing the priors does not change the parameter values much.

:::: {.content-hidden when-format="pdf"}
::: {.callout-warning collapse=true}
```{r echo=FALSE}
fit1a <- readRDS("fit1a.rds")
fit1b <- readRDS("fit1b.rds")

saveRDS(fit1a, file="fit1a.rds")
saveRDS(fit1b, file="fit1b.rds")
```
:::: 
:::

As shown in Figure 10 and Figure 11 of the posterior predictive visual check below, the light blue curves, which represent the replicated data sets from the posterior predictive distribution, does not change dramatically. This behavior suggests that our linear model is not sensitive to our changes in priors. 

```{r fig.cap = "Posterior predictive check for the linear model with new priors", echo=FALSE}
brms::pp_check(fit1a)
brms::pp_check(fit1b)
```


# Hierarchical model

The structure of our analysis with the non-linear model is similar to that of the linear model. We first fit the model, then report convergence diagnostics, and posterior predictive checks. We use the same priors for the hierarchical model as for the non-hierarchical one.

## MCMC inference for the hierarchical model

The MCMC was run by the defaults of the `brm()` function: 4 chains with 2000 iterations and warmup constituting 50 % of the draws. However, the default adaptive step size of 0.8 was changed to +.9, because the runs with the default resulted in divergent transitions.

```{r}
fit2 =  brms::brm(bf(Price ~ b1*Size + b2*Age + b3*Age^2 + b4,
                   b1 ~ 1, 
                   b2 ~ 1 + (1|PostalCode),
                   b3 ~ 1 + (1|PostalCode),
                   b4 ~ 1 + (1|PostalCode),
                   nl = TRUE
                   ),
           data = HouseData,
           family = gaussian(),
           prior = c(
             prior(normal(5000, 1500), nlpar = 'b1'),
             prior(normal(-1000, 5000), nlpar = 'b2'),
             prior(normal(0, 1000), nlpar = 'b3'),
             prior(normal(100000, 30000), nlpar = 'b4')
             ),
           refresh = 0,
           show_exceptions = FALSE,
           # This causes brms to cache the results
           file="fit2.rds"
           )
```

:::: {.content-hidden when-format="pdf"}
::: {.callout-warning collapse=true}

```{r}
fit2 <- readRDS("fit2.rds")
```

```{r}
saveRDS(fit2, file="fit2.rds")
```
:::: 
::: 

## Convergence diagnostic 

Below is the summary of the fitted hierarchical model. Again, we are checking $\hat{R}$ values and effective sample sizes to diagnose convergence:

```{r echo=FALSE}
summary(fit2)
```

As shown in the summary above, the $\hat{R}$ value is under 1.01 for every parameter, effective sample sizes around 2000 and there were no divergent transitions. The $\hat{R}$'s imply convergence of the chains and the effective sample sizes seem sufficient.

Similarly, by using function `check_hmc_diagnostics()`, we can verify that none of 4000 iterations saturated the maximum tree depth of 10.

:::: {.content-hidden when-format="pdf"}
::: {.callout-warning collapse=true}
```{r echo=FALSE}
rstan::check_hmc_diagnostics(fit2$fit)
```
:::: 
:::



## Posterior predictive check
Visually (Figure 12-16) we can see that the hierarchical model gives a fairly similar result in capturing the distribution of house prices as does the non-hierarchical model, i.e., the distribution matches fairly well, but doesn't capture some areas of the distribution (especially for the houses with below-average prices).

```{r, fig.height=3, fig.width=5, fig.cap="Conditional effects of housing attributes on housing price.", echo=FALSE}
plot(brms::conditional_effects(fit2), points = TRUE)
```

```{r, fig.height=6, fig.width=8, fig.cap="Hierarchical model: Four MCMC chains and posterior distributions for each parameter."}
plot(fit2) # chain mixing & posterior distributions for parameters
```


```{r, check2, fig.cap = "Posterior predictive check for the hierarchical model", echo=FALSE}
brms::pp_check(fit2)
```

## Sensitivity analysis
Sensitivity analysis for the hierarchical model was skipped because the sensitivities of the parameters on the selected priors ought to match those of the linear model. In addition, there are too many parameters to render any visual presentation of the results a bore, and the hierarchical model doesn't seem to be that much better that there would be any utility in completing sensitivity analysis for it.


# Model comparison 

We start our model comparison by using leave-one-out cross-validation (LOO-CV). The better model has higher `elpd_loo` (better predictive model for observed data) and higher `p_loo` values (more complex model). 


Below is the leave-one-out cross-validation for the non-hierarchical linear model by using the `loo()` function:
```{r echo=FALSE}
loo1 <- loo(fit1)    
loo1
```

Next, we apply the same `loo()` function for the hierarchical model:
```{r echo=FALSE}
loo2 <- loo(fit2) 
loo2
```


`elpd_loo` values of two models are not too significantly different. Comparing the models via LOO-CV, we see that the hierarchical model isn't much better than the non-hierarchical one. `p_loo` values of the hierarchical model are significantly higher which means that it is much more complex than the non-hierarchical model.

```{r}
loo_compare(loo1, loo2)
```

:::: {.content-hidden when-format="pdf"}
::: {.callout-warning collapse=false}
## Model comparison using RMSE

The lower the RMSE (Root-mean-square error), the better the model. The best model has the lowest RMSE, while the linear regression model (the worst model) has the highest RMSE.

The below function takes a brms fit object and computes either the [root-mean-square error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) or the PSIS-LOO-RMSE, i.e. the RMSE using LOO-CV estimated using PSIS-LOO.
```{r}
# Compute RMSE or LOO-RMSE
rmse <- function(fit, use_loo=FALSE){
  mean_y_pred <- if(use_loo){
    brms::loo_predict(fit)
  }else{
    colMeans(brms::posterior_predict(fit)) 
  }
  sqrt(mean(
    (mean_y_pred - brms::get_y(fit))^2
  ))
}
```



```{r}
rmse(fit1); rmse(fit2)
```
:::
::::

# Discussion

The linear model gives a fairly good model of the Espoo house price data, but it doesn't capture some aspects of the distribution, especially the positivity of house price. The shortcomings of the linear model don't seem to be ameliorated by switching to a hierarchical model with the average income per postal area as the grouping variable, i.e., the non-linearity present in the aggregate data isn't explained by heterogeneity between postal areas' average income. As an experiment, a non-linear and non-hierarchical model could be tested to see if it gives better results.

# Conclusion

Although the hierarchical model is more complex, it doesn't provide much added benefit in comparison to the non-hierarchical one. The linear model in itself is a fairly good model given that it is simple and fast to fit and adjust. However, a non-linear model would be a good experiment for further analysis and might give better results.

# Self-reflection
We learned how to form a Bayesian analysis problem. We revised the non-hierarchical and hierarchical models covered in this course. We also learned how to manage time, structure the report to make it readable and easy to follow.

# References

::: {#refs}
:::

# Appendix